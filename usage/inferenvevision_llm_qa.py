# -*- coding: utf-8 -*-
"""InferenveVision_LLM_QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17pE0hM-toJCF_600Ct_coTsVk0gM2o1c

<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/main/assets/Inference%20Vision%20Cover.png" alt="github.com/doguilmak/InferenceVision"/>

# **Introduction**

In this notebook, we demonstrate how to fine-tune and utilize the `EleutherAI/pythia-1b` model for question-answering (Q&A) tasks using the InferenceVision library. This guide will walk you through the process of setting up the necessary environment, loading the pre-trained model, and interacting with it for Q&A purposes.

<br>

## **Model Information**

The `EleutherAI/pythia-1b` model is a powerful architecture from the Pythia series, designed to handle a variety of natural language processing tasks. Its substantial number of parameters allows it to capture complex patterns and nuances in text, making it suitable for fine-tuning on custom Q&A tasks.

<br>


### Key Features:
- **Large Capacity**: With a significant number of parameters, this model is capable of understanding and generating complex responses.
- **Versatility**: It can be adapted to various NLP tasks, including question answering and text generation.
- **Efficiency**: The model's architecture ensures effective handling of large-scale data and intricate language patterns.

## **Setting Up the Environment**

To get started with fine-tuning and using the model, follow these steps:

1. **Install Dependencies**:
   - Ensure that you have the necessary libraries installed. For this notebook, we use `gdown` to download model files and `transformers` to handle the model and tokenizer.

2. **Download Model Weights and Tokens**:
   - Use the provided code block to download the pre-trained model weights and tokenizer from Google Drive. The model files are then unzipped and prepared for use.

3. **Load Pre-trained Model**:
   - The `AutoModelForCausalLM` and `AutoTokenizer` from the `transformers` library are used to load the fine-tuned model and tokenizer.

4. **Set Device**:
   - Determine whether to use GPU or CPU based on availability. The model is loaded onto the selected device to ensure efficient processing.

<br>

## **Interacting with the Model**

Once the environment is set up and the model is loaded, you can start interacting with it. The notebook includes an interactive chat component that allows you to input questions and receive answers generated by the model. You can exit the conversation with typing `exit`.

```
while True:
    user_question = input("Enter your question (or type 'exit' to stop): ")

    if user_question.lower() == 'exit':
        print("Exiting the chat. Goodbye!")
        break

    user_answer = answer_question(user_question)
    print(f"\nAnswer: {user_answer}\n")
```
"""

# @markdown **Creating the Necessary Environment**

# @markdown To to download the LLM weights and tokens, please run the code block.
!pip install gdown -q

import gdown

file_id = '17HVYW6JOUhOQ6f9tLBdoJwpij_Ouca2N'
url = f'https://drive.google.com/uc?id={file_id}'
output = 'inferencevision_docs.jsonl_16_steps.zip'

gdown.download(url, output, quiet=True)

!unzip -q inferencevision_docs.jsonl_16_steps.zip
output_dir = '/content/inferencevision_docs.jsonl_16_steps'

# @markdown To set up the environment for using the model, follow these steps:
# @markdown 1. **Install Dependencies**: Ensure that the necessary libraries are installed.
# @markdown 2. **Load Pre-trained Model**: Use the provided script to load the model and tokenizer.
# @markdown 3. **Set Device**: Determine whether to use GPU or CPU based on availability.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

save_dir = f'{output_dir}/final'

model = AutoModelForCausalLM.from_pretrained(save_dir)
tokenizer = AutoTokenizer.from_pretrained(save_dir)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

def answer_question(question):

    inputs = tokenizer(question, return_tensors="pt")
    input_ids = inputs.input_ids.to(device)
    attention_mask = inputs.attention_mask.to(device)

    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=50)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return answer

"""The `EleutherAI/pythia-1b` is part of the Pythia series, which is designed to handle a wide range of natural language processing tasks with substantial capacity and efficiency. The model's large number of parameters enables it to capture intricate patterns and nuances in the data, making it suitable for fine-tuning on custom Q&A tasks.

<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/refs/heads/main/assets/qa_top.png" height=200 width=1000 alt="github.com/doguilmak/InferenceVision"/>
"""

# @markdown Run the code and interact with our advanced language model for conversations!
while True:

    user_question = input("Enter your question (or type 'exit' to stop): ")

    if user_question.lower() == 'exit':
        print("Exiting the chat. Goodbye!")
        break

    user_answer = answer_question(user_question)
    print(f"\nAnswer: {user_answer}\n")

"""<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/refs/heads/main/assets/qa_bottom.png" height=100 width=1500 alt="github.com/doguilmak/InferenceVision"/>"""