# -*- coding: utf-8 -*-
"""InferenveVision_LLM_QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/135sriBzgj4HdAn_iBguH9dhp_czilIZj

<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/main/assets/Inference%20Vision%20Cover.png" alt="github.com/doguilmak/InferenceVision"/>

# Introduction

This notebook demonstrates how to leverage large language models (LLMs) using the **InferenceVision** framework — now improved with a modular, clean design using the `chat_llm.py` helper module. The helper abstracts model loading, question answering, and interactive chatting, making your workflows simpler and more maintainable. Script demonstrates how to clone the fine-tuned InferenceVision LLM from Hugging Face, load the model and tokenizer, and interactively query it.


<br>

## Model Information

This notebook supports two powerful language models within the InferenceVision framework: `pythia-1b` and `gpt-neo-1.3B`.

- The **`pythia-1b`** model, part of the Pythia series, features around 1 billion parameters and is designed to capture complex language patterns and subtle contextual nuances. It excels at a variety of NLP tasks such as question answering and text generation, and can be fine-tuned on custom datasets to specialize in specific domains.

- The **`gpt-neo-1.3B`** model is a variant of the GPT-Neo architecture, with approximately 1.3 billion parameters. It offers a complementary approach with a robust transformer design optimized for generating coherent and contextually relevant responses across many NLP applications, including conversational AI and summarization.

<br>

Both models are integrated with InferenceVision’s modular system, enabling easy loading, switching, and interaction through the `chat_llm.py` helper module. This setup allows users to choose the best model for their needs based on performance, capacity, and available resources.


<br>


### Key Features:

- **Modular Helper Module:** All the complex logic for loading models and generating answers is encapsulated in `chat_llm.py`. This keeps your notebooks cleaner and easier to read.
- **Multi-Model Support:** Easily switch between different LLMs, such as `inferencevision-pythia-1b` and `inferencevision-gpt-neo-1.3B`.
- **Concise Answer Mode:** Toggle between detailed or short answers for flexible responses.
- **Thread-Safe Loading:** Avoid redundant loading when switching models.
- **Interactive REPL:** A simple interactive loop lets you chat with the model live, with commands to customize behavior.

## Interacting with the Model

Once the environment is set up and the model is loaded, you can start interacting with it. The notebook includes an interactive chat component that allows you to input questions and receive answers generated by the model.

Make sure your runtime is **GPU** (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`.
"""

!nvidia-smi

from chat_llm import load_model, ask, loop

"""## Pythia Model

The **inferencevision-pythia-1b** model is part of the Pythia series, which consists of large-scale transformer-based language models designed for versatile and efficient natural language understanding. With around **1 billion parameters**, it balances strong language comprehension with manageable computational requirements.

This model excels at a wide range of NLP tasks such as question answering, text generation, and conversational AI. Its architecture enables it to capture complex linguistic structures and contextual nuances, making it suitable for fine-tuning on specialized datasets for domain-specific applications.
"""

load_model("inferencevision-pythia-1B")

"""<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/refs/heads/main/assets/qa_top.png" height=200 width=1000 alt="github.com/doguilmak/InferenceVision"/>"""

# response = ask("What is InferenceVision?", short=True)  # For a concise answer
response = ask("What is InferenceVision?")
print("Model (pythia-1b) response:", response)

# Start an interactive Q&A session
loop()

"""<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/refs/heads/main/assets/qa_bottom.png" height=100 width=1500 alt="github.com/doguilmak/InferenceVision"/>

## GPT-Neo Model

The **inferencevision-gpt-neo-1.3B** model is based on the GPT-Neo architecture and features approximately **1.3 billion parameters**. It offers an alternative transformer design known for generating coherent, contextually rich responses, and is well-suited for tasks like conversational AI, summarization, and general-purpose text generation.

This model is particularly useful when a slightly larger capacity is desired to handle more complex or nuanced queries, while still maintaining reasonable inference speed on available hardware.
"""

load_model("inferencevision-gpt-neo-1.3B")

"""<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/refs/heads/main/assets/qa_top.png" height=200 width=1000 alt="github.com/doguilmak/InferenceVision"/>"""

# response = ask("What is InferenceVision?", short=True)  # For a concise answer
response = ask("What is InferenceVision?")
print("Model (GPT-Neo 1.3B) response:", response)

# Start an interactive Q&A session
loop()

"""<img  src="https://raw.githubusercontent.com/doguilmak/InferenceVision/refs/heads/main/assets/qa_bottom.png" height=100 width=1500 alt="github.com/doguilmak/InferenceVision"/>"""