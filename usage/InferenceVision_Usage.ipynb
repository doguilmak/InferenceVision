{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/doguilmak/InferenceVision/main/assets/Inference%20Vision%20Cover.png\" alt=\"github.com/doguilmak/InferenceVision\"/>\n",
        "\n",
        "In contemporary scientific research and applications, there is an increasing demand for accurate geospatial analysis to address various real-world challenges, ranging from environmental monitoring to urban planning and disaster response. The ability to precisely locate and identify objects within geographic areas plays a pivotal role in such endeavors. In this scientific project, we aim to enhance geospatial analysis by integrating object detection techniques with geographic coordinate calculations."
      ],
      "metadata": {
        "id": "1n5JQhFNLH8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "Traditional methods of geospatial analysis often rely on manual identification and mapping of objects within geographical regions. However, these methods are time-consuming, labor-intensive, and prone to errors. Moreover, they may lack the scalability required for large-scale analyses. Therefore, there is a need for automated solutions that can accurately detect and locate objects within geographic areas, enabling efficient and scalable geospatial analysis."
      ],
      "metadata": {
        "id": "VnYP3VDTK0iG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Objective\n",
        "\n",
        "Our project seeks to address the aforementioned challenges by developing an automated system that combines object detection algorithms with geographic coordinate calculations. By integrating these components, we aim to achieve the following objectives:\n",
        "\n",
        "<br>\n",
        "\n",
        "1. **Object Detection:** Utilize state-of-the-art object detection algorithms, such as YOLO (You Only Look Once), to automatically identify and localize objects within satellite or aerial imagery.\n",
        "\n",
        "2. **Geographic Coordinate Calculation:** Develop algorithms to calculate the geographic coordinates (latitude and longitude) of detected objects relative to a given bounding polygon. This involves converting normalized center coordinates of objects within the bounding polygon to precise geographic coordinates.\n",
        "\n",
        "3. **Integration and Visualization:** Integrate object detection results with calculated geographic coordinates to create a comprehensive geospatial dataset. Visualize the detected objects and their geographic locations on maps for further analysis and interpretation."
      ],
      "metadata": {
        "id": "TD2qgxh0K3iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology\n",
        "\n",
        "In this section, we outline the methodology employed for deriving geographic coordinates from input data within the InferenceVision framework. This methodological approach combines advanced techniques in satellite image analysis, object detection, and geographic coordinate calculation to enable precise geospatial analysis and visualization. Let's delve into the steps involved:\n",
        "\n",
        "**Given a set of inputs, the calculation unfolds as follows:**\n",
        "\n",
        "<br>\n",
        "\n",
        "**1- Transform VHR Satellite Image Coordinates to WGS 84 (EPSG:4326) and Extract Polygon Coordinates:** The target Coordinate Reference System (CRS) is WGS 84, representing a geographic coordinate system. Converting to this CRS standardizes the data. We use Nearest Neighbor interpolation, which can result in a blocky appearance. Transformed coordinates are precise to 9 decimal places (as default). First, we transform image coordinates to WGS 84. The coordinates of the polygons are defined as G (geometric shapes) and the following transformation operations are applied to convert these shapes to the geographic coordinate system:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ G_{EPSG:4326} = transform(G_{dataset}, CRS_{dataset}) $$\n",
        "\n",
        "<br>\n",
        "\n",
        "Then, we extract polygon coordinates, defining the geographical extent with top-left ($TL$) and bottom-right ($BR$) corners as reference points for computing the geographic coordinates of normalized centers.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "**2- Calculate Normalized Centers:** In the second stage, model making prediction and making detections. Then, the center coordinates of the detected objects are calculated from their bounding boxes. The edge coordinates determined for each object ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$) are used. $x_{min}$ and $x_{max}$ are the pixel coordinates of the left and right edges of the bounding boxes on the x-axis, and $y_{min}$ and $y_{max}$ are the pixel coordinates of the top and bottom edges of the bounding boxes on the y-axis. The center point of the object is determined by the following formula:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ (x_{center}, y_{center}) = (\\frac{x_{min} + x_{max}}{2} + \\frac{y_{min}+y_{max}}{2})$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The centroids of the bounding boxes are then normalized, which is necessary to convert the pixel coordinates of object locations within the image into a standard format. Normalization can be expressed as:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$N_x = \\frac{x_{center}}{W}$$\n",
        "\n",
        "$$N_y = \\frac{y_{center}}{H}$$\n",
        "\n",
        "Where:\n",
        "- **$N_x$**: The value of the normalized pixel coordinate of the center point along the x-axis.\n",
        "- **$N_y$**: The value of the normalized pixel coordinate of the center point along the y-axis.\n",
        "- **$X_{center}$**: The x pixel coordinate of the center of the bounding box.\n",
        "- **$Y_{center}$**: The y pixel coordinate of the center of the bounding box.\n",
        "- **$W$**: The total width of the raster image.\n",
        "- **$H$**: The total height of the raster image.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "**3- Calculate Geographic Coordinates:** Finally, the geographic coordinates are calculated using the normalized center coordinates. In this stage, the corner coordinates of the extracted polygon are taken as reference. The normalized values ​​are used to determine the actual locations on the geographic area by associating them with these corner coordinates. The calculation is carried out with the following formulas:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ lat = lat_{TL} + (lat_{BR} - lat_{TL}) \\times N_{x} $$\n",
        "\n",
        "$$ lon = lon_{TL} + (lon_{BR} - lon_{TL}) \\times N_{y} $$\n",
        "\n",
        "   \n",
        "   **Where:**\n",
        "\n",
        "   - $lat$ represents latitude.\n",
        "   - $lon$ represents longitude.\n",
        "   - $N_{x}$ and $N_{y}$ are the normalized center coordinates.\n",
        "   - $lat_{TL}, lon_{TL}, lat_{BR},$ and $lon_{BR}$ are the latitude and longitude of the top-left and bottom-right corners of the polygon, respectively."
      ],
      "metadata": {
        "id": "bvqRNtMeK8JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scientific Significance\n",
        "The proposed project has several scientific implications and contributions:\n",
        "\n",
        "- **Automation and Efficiency:** By automating the process of object detection and geographic coordinate calculation, our system significantly reduces the time and effort required for geospatial analysis, thereby enhancing efficiency and scalability.\n",
        "\n",
        "- **Accuracy and Precision:** Through the integration of advanced algorithms, our system ensures high accuracy and precision in object detection and geographic coordinate calculation, leading to reliable and trustworthy results.\n",
        "\n",
        "- **Versatility and Adaptability:** The developed system is versatile and adaptable to various applications, including environmental monitoring, urban planning, agriculture, and disaster response. It provides researchers and practitioners with a powerful tool for analyzing geospatial data in diverse contexts."
      ],
      "metadata": {
        "id": "g3BzfCciLAYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## 1. How to Use"
      ],
      "metadata": {
        "id": "oZzM9QSfKf1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using Colab environment, make sure your runtime is **GPU** (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`. To utilize the `InferenceVision` class for object detection and geographic coordinate calculation, follow these steps:"
      ],
      "metadata": {
        "id": "74sLg_SZj1yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### 1.1. Clone and Install"
      ],
      "metadata": {
        "id": "kXOdutO6sBih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/doguilmak/InferenceVision.git\n",
        "%cd InferenceVision"
      ],
      "metadata": {
        "id": "9WNc9hi4Q1d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5677c6b9-b236-4ad3-91e5-e2784c8aacc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'InferenceVision'...\n",
            "remote: Enumerating objects: 475, done.\u001b[K\n",
            "remote: Counting objects: 100% (367/367), done.\u001b[K\n",
            "remote: Compressing objects: 100% (266/266), done.\u001b[K\n",
            "remote: Total 475 (delta 226), reused 171 (delta 97), pack-reused 108 (from 1)\u001b[K\n",
            "Receiving objects: 100% (475/475), 20.74 MiB | 24.66 MiB/s, done.\n",
            "Resolving deltas: 100% (267/267), done.\n",
            "/content/InferenceVision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxqd97d80yi-",
        "outputId": "40bd0606-c5b5-471f-eb08-ef856efc53d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.3/760.3 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### 1.2. Initialization"
      ],
      "metadata": {
        "id": "UxURjcp4KmVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize an instance of the `InferenceVision` class by providing the following parameters:\n",
        "- `tif_path`: Path to the input TIFF image file.\n",
        "- `model_path`: Path to the YOLO model file for object detection."
      ],
      "metadata": {
        "id": "tJdI6QlRLWWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from inference_vision import InferenceVision\n",
        "\n",
        "print(f\"InferenceVision version: {InferenceVision.VERSION}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwF0PlGWNjLg",
        "outputId": "32950756-e444-40e2-d8c7-0c10d4b8ad63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InferenceVision version: 1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(InferenceVision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1lxVyfbwKW8",
        "outputId": "4789bcfe-3710-4824-9a52-3adddb1ee22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class InferenceVision in module inference_vision:\n",
            "\n",
            "class InferenceVision(builtins.object)\n",
            " |  InferenceVision(tif_path, model_path, coord_precision=9)\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, tif_path, model_path, coord_precision=9)\n",
            " |      Initialize an InferenceVision instance with configurable precision.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      tif_path : str\n",
            " |          The file path to the TIFF image to be processed.\n",
            " |      model_path : str\n",
            " |          The file path to the YOLO model weights.\n",
            " |      coord_precision : int, optional\n",
            " |          The number of decimal places to use for geographic coordinates. Default is 9.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      None\n",
            " |          This method initializes the object and does not return any value.\n",
            " |      \n",
            " |      Example\n",
            " |      -------\n",
            " |      >>> iv = InferenceVision(\"image.tif\", \"model.pt\", coord_precision=6)\n",
            " |  \n",
            " |  calculate_bbox_center(self, coordinates)\n",
            " |      Calculate the center of a bounding box.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      coordinates : list of float\n",
            " |          A list of four values [x_min, y_min, x_max, y_max] representing the bounding box.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      tuple\n",
            " |          The (x_center, y_center) of the bounding box.\n",
            " |      \n",
            " |      Example\n",
            " |      -------\n",
            " |      >>> iv = InferenceVision(\"image.tif\", \"model.pt\")\n",
            " |      >>> center = iv.calculate_bbox_center([50, 50, 150, 150])\n",
            " |      >>> print(center)\n",
            " |  \n",
            " |  normalize_center_points(self, center_points)\n",
            " |      Normalize the center points based on image dimensions.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      center_points : list of tuples\n",
            " |          A list of (x, y) center coordinates.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of tuples\n",
            " |          A list of normalized (x, y) center coordinates.\n",
            " |      \n",
            " |      Example\n",
            " |      -------\n",
            " |      >>> iv = InferenceVision(\"image.tif\", \"model.pt\")\n",
            " |      >>> normalized_points = iv.normalize_center_points([(100, 200), (150, 250)])\n",
            " |      >>> print(normalized_points)\n",
            " |  \n",
            " |  process_image(self, build_csv=False, csv_filename=None)\n",
            " |      Process the image using the YOLO model and optionally save the results to a CSV file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      build_csv : bool, optional\n",
            " |          Whether to build and save the CSV file with results. If True, the CSV file\n",
            " |          will be saved with the name provided in csv_filename. Default is False.\n",
            " |      csv_filename : str, optional\n",
            " |          The name of the CSV file to save results, if build_csv is True.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      None\n",
            " |          If build_csv is True, saves results to the specified CSV file. If False,\n",
            " |          prints results.\n",
            " |      \n",
            " |      Example\n",
            " |      -------\n",
            " |      Process the image and save results to a CSV file:\n",
            " |      >>> iv = InferenceVision(\"image.tif\", \"model.pt\")\n",
            " |      >>> iv.process_image(build_csv=True, csv_filename=\"results.csv\")\n",
            " |      \n",
            " |      Process the image and print results to the console:\n",
            " |      >>> iv = InferenceVision(\"image.tif\", \"model.pt\")\n",
            " |      >>> iv.process_image()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  VERSION = '1.2'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: The input image must have a CRS set to ensure accurate geographic coordinate calculation.**"
      ],
      "metadata": {
        "id": "2GPP6gw8VLhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iv = InferenceVision(\n",
        "    tif_path=\"/content/image.tif\", # Path to your image.\n",
        "    model_path=\"/content/model.pt\" # Path to your model.\n",
        "    # coord_precision=6 # Number of decimal places for geographic coordinates. Default is 9.\n",
        ")"
      ],
      "metadata": {
        "id": "cTH_vUkLOcLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### 1.3. Process Image"
      ],
      "metadata": {
        "id": "ci6ibyHXLnUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoke the `process_image` method to perform object detection and geographic coordinate calculation on the input image. Using GPU can speed up the process."
      ],
      "metadata": {
        "id": "m7ms6SuFLt-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the image and save results to a CSV file\n",
        "csv_filename = \"results.csv\"\n",
        "iv.process_image(build_csv=True, csv_filename=csv_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToJlDc-wLlIj",
        "outputId": "fbd0f365-f632-45e6-807d-34c786f17567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/image.tif: 640x640 5 collapseds, 10 non_collapseds, 82.1ms\n",
            "Speed: 7.5ms preprocess, 82.1ms inference, 1857.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "DataFrame saved as results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method detects objects in the input image, calculates their geographic coordinates, and optionally saves the results to a CSV file."
      ],
      "metadata": {
        "id": "bhp-81MlMBID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the image and print results to the console\n",
        "iv.process_image()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hNOmSMVMSWw",
        "outputId": "a1ba48cb-743e-497a-9373-b63d4116e80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:732: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(file, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/image.tif: 640x640 5 collapseds, 10 non_collapseds, 96.3ms\n",
            "Speed: 1.6ms preprocess, 96.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "Point 0 --------------------\n",
            "Latitude: 36.209461282 | Longitude: 36.152872430\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [23.909332275390625, 87.95037841796875, 101.81744384765625, 148.59400939941406]\n",
            "Confidence Score: 0.8144\n",
            "Bounding Box Center (X, Y): (62.86338806152344, 118.2721939086914)\n",
            "Normalized Bounding Box Center (X, Y): [0.09822404384613037, 0.18480030298233033]\n",
            "\n",
            "Point 1 --------------------\n",
            "Latitude: 36.208325574 | Longitude: 36.153734978\n",
            "Object Type: collapsed\n",
            "Coordinates (Bounding Box): [267.8592529296875, 502.86468505859375, 352.1070556640625, 585.2774047851562]\n",
            "Confidence Score: 0.7816\n",
            "Bounding Box Center (X, Y): (309.983154296875, 544.071044921875)\n",
            "Normalized Bounding Box Center (X, Y): [0.4843486785888672, 0.8501110076904297]\n",
            "\n",
            "Point 2 --------------------\n",
            "Latitude: 36.208897244 | Longitude: 36.153386582\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [172.59658813476562, 296.422607421875, 247.73904418945312, 363.05908203125]\n",
            "Confidence Score: 0.7746\n",
            "Bounding Box Center (X, Y): (210.16781616210938, 329.7408447265625)\n",
            "Normalized Bounding Box Center (X, Y): [0.3283872127532959, 0.5152200698852539]\n",
            "\n",
            "Point 3 --------------------\n",
            "Latitude: 36.208666423 | Longitude: 36.153117744\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [93.84264373779297, 380.179931640625, 172.44915771484375, 452.3802490234375]\n",
            "Confidence Score: 0.7676\n",
            "Bounding Box Center (X, Y): (133.14590072631836, 416.28009033203125)\n",
            "Normalized Bounding Box Center (X, Y): [0.20804046988487243, 0.6504376411437989]\n",
            "\n",
            "Point 4 --------------------\n",
            "Latitude: 36.209661981 | Longitude: 36.153168488\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [107.0134048461914, 9.3819580078125, 188.35470581054688, 76.6705551147461]\n",
            "Confidence Score: 0.7640\n",
            "Bounding Box Center (X, Y): (147.68405532836914, 43.0262565612793)\n",
            "Normalized Bounding Box Center (X, Y): [0.23075633645057678, 0.0672285258769989]\n",
            "\n",
            "Point 5 --------------------\n",
            "Latitude: 36.208308743 | Longitude: 36.153344888\n",
            "Object Type: collapsed\n",
            "Coordinates (Bounding Box): [153.92172241210938, 507.59808349609375, 242.52365112304688, 593.1640014648438]\n",
            "Confidence Score: 0.7586\n",
            "Bounding Box Center (X, Y): (198.22268676757812, 550.3810424804688)\n",
            "Normalized Bounding Box Center (X, Y): [0.3097229480743408, 0.8599703788757325]\n",
            "\n",
            "Point 6 --------------------\n",
            "Latitude: 36.209557386 | Longitude: 36.153669699\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [248.67974853515625, 44.954742431640625, 333.8822021484375, 119.52664184570312]\n",
            "Confidence Score: 0.7400\n",
            "Bounding Box Center (X, Y): (291.2809753417969, 82.24069213867188)\n",
            "Normalized Bounding Box Center (X, Y): [0.4551265239715576, 0.1285010814666748]\n",
            "\n",
            "Point 7 --------------------\n",
            "Latitude: 36.209727314 | Longitude: 36.154613049\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [519.956298828125, 0.0, 603.144287109375, 37.063228607177734]\n",
            "Confidence Score: 0.7391\n",
            "Bounding Box Center (X, Y): (561.55029296875, 18.531614303588867)\n",
            "Normalized Bounding Box Center (X, Y): [0.8774223327636719, 0.028955647349357606]\n",
            "\n",
            "Point 8 --------------------\n",
            "Latitude: 36.208744259 | Longitude: 36.152762034\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [0.0, 353.341064453125, 62.470062255859375, 420.85455322265625]\n",
            "Confidence Score: 0.7376\n",
            "Bounding Box Center (X, Y): (31.235031127929688, 387.0978088378906)\n",
            "Normalized Bounding Box Center (X, Y): [0.048804736137390135, 0.6048403263092041]\n",
            "\n",
            "Point 9 --------------------\n",
            "Latitude: 36.208936709 | Longitude: 36.153974301\n",
            "Object Type: collapsed\n",
            "Coordinates (Bounding Box): [337.58038330078125, 268.1007080078125, 419.51806640625, 361.78839111328125]\n",
            "Confidence Score: 0.7253\n",
            "Bounding Box Center (X, Y): (378.5492248535156, 314.9445495605469)\n",
            "Normalized Bounding Box Center (X, Y): [0.5914831638336182, 0.4921008586883545]\n",
            "\n",
            "Point 10 --------------------\n",
            "Latitude: 36.209198872 | Longitude: 36.153744760\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [275.05548095703125, 183.66070556640625, 350.51611328125, 249.64877319335938]\n",
            "Confidence Score: 0.7109\n",
            "Bounding Box Center (X, Y): (312.7857971191406, 216.6547393798828)\n",
            "Normalized Bounding Box Center (X, Y): [0.4887278079986572, 0.3385230302810669]\n",
            "\n",
            "Point 11 --------------------\n",
            "Latitude: 36.208145137 | Longitude: 36.154419618\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [456.84033203125, 583.4404907226562, 555.4246826171875, 640.0]\n",
            "Confidence Score: 0.6428\n",
            "Bounding Box Center (X, Y): (506.13250732421875, 611.7202453613281)\n",
            "Normalized Bounding Box Center (X, Y): [0.7908320426940918, 0.9558128833770752]\n",
            "\n",
            "Point 12 --------------------\n",
            "Latitude: 36.208941674 | Longitude: 36.154658539\n",
            "Object Type: collapsed\n",
            "Coordinates (Bounding Box): [521.4935302734375, 266.5304260253906, 627.6729736328125, 359.6360778808594]\n",
            "Confidence Score: 0.5695\n",
            "Bounding Box Center (X, Y): (574.583251953125, 313.083251953125)\n",
            "Normalized Bounding Box Center (X, Y): [0.8977863311767578, 0.4891925811767578]\n",
            "\n",
            "Point 13 --------------------\n",
            "Latitude: 36.209553352 | Longitude: 36.154561654\n",
            "Object Type: non_collapsed\n",
            "Coordinates (Bounding Box): [501.4018859863281, 57.55866241455078, 592.249267578125, 109.9480209350586]\n",
            "Confidence Score: 0.5631\n",
            "Bounding Box Center (X, Y): (546.8255767822266, 83.75334167480469)\n",
            "Normalized Bounding Box Center (X, Y): [0.854414963722229, 0.13086459636688233]\n",
            "\n",
            "Point 14 --------------------\n",
            "Latitude: 36.208642695 | Longitude: 36.153731039\n",
            "Object Type: collapsed\n",
            "Coordinates (Bounding Box): [265.8543701171875, 379.77777099609375, 351.8551025390625, 470.57440185546875]\n",
            "Confidence Score: 0.5132\n",
            "Bounding Box Center (X, Y): (308.854736328125, 425.17608642578125)\n",
            "Normalized Bounding Box Center (X, Y): [0.4825855255126953, 0.6643376350402832]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### 1.4. Interpret Results"
      ],
      "metadata": {
        "id": "-zY2E1BUMEIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the image processing is complete, you can interpret the results either by printing them to the console or by analyzing the generated CSV file. Now, let's load CSV file into a DataFrame."
      ],
      "metadata": {
        "id": "qM-CNNDgMJ4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"results.csv\")"
      ],
      "metadata": {
        "id": "zjb7oIMeMByy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze and visualize the results."
      ],
      "metadata": {
        "id": "EXnyXcLkSIcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "IerHZDFTMZFD",
        "outputId": "6edaaf0b-cd40-42ad-8049-8c5d740d5f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Image  Point   Latitude  Longitude    Object Type  \\\n",
              "0  /content/image.tif      0  36.209461  36.152872  non_collapsed   \n",
              "1  /content/image.tif      1  36.208326  36.153735      collapsed   \n",
              "2  /content/image.tif      2  36.208897  36.153387  non_collapsed   \n",
              "3  /content/image.tif      3  36.208666  36.153118  non_collapsed   \n",
              "4  /content/image.tif      4  36.209662  36.153168  non_collapsed   \n",
              "\n",
              "                                         Coordinates  Confidence Score  \\\n",
              "0  [23.909332275390625, 87.95037841796875, 101.81...          0.814360   \n",
              "1  [267.8592529296875, 502.86468505859375, 352.10...          0.781579   \n",
              "2  [172.59658813476562, 296.422607421875, 247.739...          0.774597   \n",
              "3  [93.84264373779297, 380.179931640625, 172.4491...          0.767572   \n",
              "4  [107.0134048461914, 9.3819580078125, 188.35470...          0.763968   \n",
              "\n",
              "                        Bounding Box Center  \\\n",
              "0    [62.86338806152344, 118.2721939086914]   \n",
              "1      [309.983154296875, 544.071044921875]   \n",
              "2   [210.16781616210938, 329.7408447265625]   \n",
              "3  [133.14590072631836, 416.28009033203125]   \n",
              "4    [147.68405532836914, 43.0262565612793]   \n",
              "\n",
              "               Normalized Bounding Box Center  \n",
              "0  [0.09822404384613037, 0.18480030298233033]  \n",
              "1    [0.4843486785888672, 0.8501110076904297]  \n",
              "2    [0.3283872127532959, 0.5152200698852539]  \n",
              "3   [0.20804046988487243, 0.6504376411437989]  \n",
              "4   [0.23075633645057678, 0.0672285258769989]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa0308f8-66a4-42a4-90a7-73c71b7c4124\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Point</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>Object Type</th>\n",
              "      <th>Coordinates</th>\n",
              "      <th>Confidence Score</th>\n",
              "      <th>Bounding Box Center</th>\n",
              "      <th>Normalized Bounding Box Center</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/image.tif</td>\n",
              "      <td>0</td>\n",
              "      <td>36.209461</td>\n",
              "      <td>36.152872</td>\n",
              "      <td>non_collapsed</td>\n",
              "      <td>[23.909332275390625, 87.95037841796875, 101.81...</td>\n",
              "      <td>0.814360</td>\n",
              "      <td>[62.86338806152344, 118.2721939086914]</td>\n",
              "      <td>[0.09822404384613037, 0.18480030298233033]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/image.tif</td>\n",
              "      <td>1</td>\n",
              "      <td>36.208326</td>\n",
              "      <td>36.153735</td>\n",
              "      <td>collapsed</td>\n",
              "      <td>[267.8592529296875, 502.86468505859375, 352.10...</td>\n",
              "      <td>0.781579</td>\n",
              "      <td>[309.983154296875, 544.071044921875]</td>\n",
              "      <td>[0.4843486785888672, 0.8501110076904297]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/image.tif</td>\n",
              "      <td>2</td>\n",
              "      <td>36.208897</td>\n",
              "      <td>36.153387</td>\n",
              "      <td>non_collapsed</td>\n",
              "      <td>[172.59658813476562, 296.422607421875, 247.739...</td>\n",
              "      <td>0.774597</td>\n",
              "      <td>[210.16781616210938, 329.7408447265625]</td>\n",
              "      <td>[0.3283872127532959, 0.5152200698852539]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/image.tif</td>\n",
              "      <td>3</td>\n",
              "      <td>36.208666</td>\n",
              "      <td>36.153118</td>\n",
              "      <td>non_collapsed</td>\n",
              "      <td>[93.84264373779297, 380.179931640625, 172.4491...</td>\n",
              "      <td>0.767572</td>\n",
              "      <td>[133.14590072631836, 416.28009033203125]</td>\n",
              "      <td>[0.20804046988487243, 0.6504376411437989]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/image.tif</td>\n",
              "      <td>4</td>\n",
              "      <td>36.209662</td>\n",
              "      <td>36.153168</td>\n",
              "      <td>non_collapsed</td>\n",
              "      <td>[107.0134048461914, 9.3819580078125, 188.35470...</td>\n",
              "      <td>0.763968</td>\n",
              "      <td>[147.68405532836914, 43.0262565612793]</td>\n",
              "      <td>[0.23075633645057678, 0.0672285258769989]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa0308f8-66a4-42a4-90a7-73c71b7c4124')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fa0308f8-66a4-42a4-90a7-73c71b7c4124 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fa0308f8-66a4-42a4-90a7-73c71b7c4124');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c30d289d-af42-4a91-af71-e6ce2a450acf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c30d289d-af42-4a91-af71-e6ce2a450acf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c30d289d-af42-4a91-af71-e6ce2a450acf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/image.tif\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Point\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.000522263803427043,\n        \"min\": 36.208145137,\n        \"max\": 36.209727314,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          36.208936709\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0006267328898143063,\n        \"min\": 36.152762034,\n        \"max\": 36.154658539,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          36.153974301\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Object Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"collapsed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Coordinates\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"[337.58038330078125, 268.1007080078125, 419.51806640625, 361.78839111328125]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Confidence Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09100221165081357,\n        \"min\": 0.513218879699707,\n        \"max\": 0.8143603801727295,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.725348174571991\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bounding Box Center\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"[378.5492248535156, 314.9445495605469]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Normalized Bounding Box Center\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"[0.5914831638336182, 0.4921008586883545]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "This calculation elucidates the process of deriving geographic coordinates from given inputs, a pivotal step within `InferenceVision` framework. It facilitates the transformation of normalized center coordinates into precise geographic coordinates, fostering accurate geospatial analysis and visualization. Geographic coordinates, namely latitude and longitude, are indispensable for pinpointing specific locations on Earth's surface. This process outlined here harmonizes normalized center coordinates, relative values within a bounding area, into a set of coordinates mappable onto a geographical map for comprehensive analysis. In conclusion, our scientific project aims to advance the field of geospatial analysis by leveraging cutting-edge technologies and methodologies. By combining object detection with geographic coordinate calculation, we strive to provide researchers and practitioners with an efficient, accurate, and versatile solution for addressing complex geospatial challenges."
      ],
      "metadata": {
        "id": "X2oIb2DxLDY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "**Debugging:** In case of any errors or unexpected behavior during image processing, carefully review the input data, model configuration, and method calls. Use debugging tools such as print statements, logging, or interactive debugging to identify and resolve issues.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Future Improvements:** Consider incorporating additional features or enhancements to further optimize the performance and usability of the `InferenceVision` class. Potential improvements may include support for alternative object detection models, integration with other geospatial libraries, or optimization of computational efficiency.\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "*Library will be available as a package on PyPI (Python Package Index).*"
      ],
      "metadata": {
        "id": "bMcTmaibXR3_"
      }
    }
  ]
}